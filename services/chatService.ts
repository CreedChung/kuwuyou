/**
 * AI èŠå¤©æœåŠ¡
 * ä½¿ç”¨ Vercel AI SDKï¼Œæ”¯æŒå¤šç§ AI æä¾›å•†
 */

import { type LanguageModel, generateText, streamText } from "ai";
import type { UserModelMessage, AssistantModelMessage, SystemModelMessage } from "ai";
import { createZhipu } from "zhipu-ai-provider";
import type { KnowledgeReference } from "../components/chat/types";

export interface ChatServiceMessage {
	role: "system" | "user" | "assistant";
	content: string;
}

export interface ChatMessage {
	role: "system" | "user" | "assistant";
	content: string;
}

export interface ChatServiceOptions {
	model?: string;
	temperature?: number;
	maxTokens?: number;
	topP?: number;
}

interface ChatCompletionChunk {
	id: string;
	created: number;
	model: string;
	choices: Array<{
		index: number;
		delta: {
			role?: string;
			content?: string;
			reasoning_content?: string;
		};
		finish_reason?: "stop" | "length" | "tool_calls" | "sensitive" | "network_error";
	}>;
	usage?: {
		prompt_tokens: number;
		completion_tokens: number;
		total_tokens: number;
	};
}

class ChatService {
	private apiKey: string;
	private baseURL: string;
	private model: string;
	private providerInstance: ReturnType<typeof createZhipu> | null = null;
	private abortController: AbortController | null = null;

	constructor(apiKey?: string, model?: string) {
		this.apiKey = apiKey || "server-side-key";
		this.baseURL =
			process.env.AI_BASE_URL ||
			"https://open.bigmodel.cn/api/paas/v4";
		this.model = model || "glm-4.5-air";
		this.initializeProvider();
	}

	/**
	 * åˆå§‹åŒ– AI Provider
	 */
	private initializeProvider() {
		if (this.apiKey) {
			this.providerInstance = createZhipu({
				baseURL: this.baseURL,
				apiKey: this.apiKey,
			});
		}
	}

	/**
	 * è®¾ç½® API Key
	 */
	setApiKey(apiKey: string) {
		this.apiKey = apiKey;
		this.initializeProvider();
	}

	/**
	 * è·å–å½“å‰ API Key
	 */
	getApiKey(): string {
		return this.apiKey;
	}

	/**
	 * æ£€æŸ¥ API Key æ˜¯å¦å·²è®¾ç½®
	 */
	hasApiKey(): boolean {
		return !!this.apiKey;
	}

	/**
	 * å°†æ¶ˆæ¯è½¬æ¢ä¸º AI SDK æ ¼å¼
	 */
	private convertMessages(messages: ChatServiceMessage[]): Array<UserModelMessage | AssistantModelMessage | SystemModelMessage> {
		return messages.map((msg) => ({
			role: msg.role,
			content: msg.content,
		})) as Array<UserModelMessage | AssistantModelMessage | SystemModelMessage>;
	}

	/**
	 * å‘é€èŠå¤©è¯·æ±‚ï¼ˆéæµå¼ï¼‰
	 */
	async chat(
		messages: ChatServiceMessage[],
		options: ChatServiceOptions = {},
	): Promise<string> {
		if (!this.providerInstance) {
			throw new Error("æœåŠ¡åˆå§‹åŒ–å¤±è´¥");
		}

		const coreMessages = this.convertMessages(messages);
		const model = this.providerInstance(options.model || "glm-4.5-air") as unknown as LanguageModel;

		const { text } = await generateText({
			model,
			messages: coreMessages,
			temperature: options.temperature,
			maxOutputTokens: options.maxTokens ?? 50000,
			topP: options.topP,
		});

		return text;
	}

	/**
	 * å‘é€èŠå¤©è¯·æ±‚ï¼ˆæµå¼å“åº”ï¼‰
	 */
	async *chatStream(
		messages: ChatServiceMessage[],
		options: ChatServiceOptions = {},
	): AsyncGenerator<string, void, unknown> {
		if (!this.providerInstance) {
			throw new Error("æœåŠ¡åˆå§‹åŒ–å¤±è´¥");
		}

		// åˆ›å»ºæ–°çš„ AbortController
		this.abortController = new AbortController();

		try {
			const coreMessages = this.convertMessages(messages);
			const model = this.providerInstance(options.model || "glm-4.5-air") as unknown as LanguageModel;

			const result = await streamText({
				model,
				messages: coreMessages,
				temperature: options.temperature,
				maxOutputTokens: options.maxTokens ?? 50000,
				topP: options.topP,
				abortSignal: this.abortController.signal,
			});

			// ä½¿ç”¨ textStream æ¥è·å–æµå¼æ–‡æœ¬
			for await (const chunk of result.textStream) {
				yield chunk;
			}
		} finally {
			this.abortController = null;
		}
	}

	/**
	 * åœæ­¢å½“å‰çš„æµå¼è¯·æ±‚
	 */
	stopStream() {
		if (this.abortController) {
			this.abortController.abort();
			this.abortController = null;
		}
	}

	/**
	 * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
	 */
	getAvailableModels(): string[] {
		return [
			"glm-4.5-air",
		];
	}

	/**
	 * è°ƒç”¨å¯¹è¯è¡¥å…¨ APIï¼ˆæµå¼ï¼‰
	 */
	async *chatCompletionStream(
		messages: ChatMessage[],
		options: {
			useKnowledge?: boolean;
			useWebSearch?: boolean;
			useThinking?: boolean;
			temperature?: number;
			maxTokens?: number;
			systemPrompt?: string;
		} = {}
	): AsyncGenerator<{
		content?: string;
		thinking?: string;
		references?: KnowledgeReference[];
		finishReason?: string;
		usage?: { prompt_tokens: number; completion_tokens: number; total_tokens: number };
	}> {
		console.log("ğŸš€ AI å¯¹è¯è¯·æ±‚å¼€å§‹");
		console.log("ğŸ“ ç”¨æˆ·æ¶ˆæ¯:", messages.filter(m => m.role === "user").map(m => m.content));
		console.log("âš™ï¸ é…ç½®:", {
			useKnowledge: options.useKnowledge,
			useWebSearch: options.useWebSearch,
			useThinking: options.useThinking,
		});

		let finalMessages = messages;
		if (options.systemPrompt) {
			const hasSystemMessage = messages.some(m => m.role === "system");
			if (!hasSystemMessage) {
				finalMessages = [
					{ role: "system", content: options.systemPrompt },
					...messages
				];
				console.log("ğŸ“‹ ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯");
			}
		}

		const requestBody: Record<string, unknown> = {
			model: this.model || "glm-4.5-air",
			messages: finalMessages,
			stream: true,
			temperature: options.temperature ?? 0.95,
			max_tokens: options.maxTokens ?? 8192,
		};

		if (options.useThinking) {
			requestBody.thinking = { type: "enabled" };
		}

		this.abortController = new AbortController();

		console.log("ğŸ”§ è¯·æ±‚ä½“:", JSON.stringify(requestBody, null, 2));

		try {
			const response = await fetch("/api/chat/completions", {
				method: "POST",
				headers: {
					"Authorization": `Bearer ${this.apiKey}`,
					"Content-Type": "application/json",
				},
				body: JSON.stringify(requestBody),
				signal: this.abortController.signal,
			});

			if (!response.ok) {
				const errorData = await response.json().catch(() => ({}));
				throw new Error(errorData.error || `è¯·æ±‚å¤±è´¥ (${response.status})`);
			}

			const reader = response.body?.getReader();
			if (!reader) {
				throw new Error("æ— æ³•è¯»å–å“åº”æµ");
			}

			const decoder = new TextDecoder();
			let buffer = "";

			while (true) {
				const { done, value } = await reader.read();
				if (done) break;

				buffer += decoder.decode(value, { stream: true });
				const lines = buffer.split("\n");
				buffer = lines.pop() || "";

				for (const line of lines) {
					if (!line.trim() || !line.startsWith("data:")) continue;

					const data = line.slice(5).trim();
					if (data === "[DONE]") {
						return;
					}

					try {
						const parsed: ChatCompletionChunk = JSON.parse(data);
						
						for (const choice of parsed.choices) {
							// å¤„ç†å®ŒæˆåŸå› 
							if (choice.finish_reason) {
								yield { finishReason: choice.finish_reason };
							}

							// å¤„ç†å¢é‡å†…å®¹
							if (choice.delta) {
								// æ€ç»´é“¾å†…å®¹
								if (choice.delta.reasoning_content) {
									yield { thinking: choice.delta.reasoning_content };
								}

								// æ™®é€šæ–‡æœ¬å†…å®¹
								if (choice.delta.content) {
									yield { content: choice.delta.content };
								}
							}
						}

						// å¤„ç† token ä½¿ç”¨ç»Ÿè®¡
						if (parsed.usage) {
							yield { usage: parsed.usage };
						}
					} catch (e) {
						console.error("âŒ è§£æSSEæ•°æ®å¤±è´¥:", e, "åŸå§‹æ•°æ®:", data);
					}
				}
			}
		} finally {
			this.abortController = null;
		}
	}

	/**
	 * è·å–è¯·æ±‚å¤´
	 */
	private getHeaders(): HeadersInit {
		return {
			Authorization: `Bearer ${this.apiKey}`,
			"Content-Type": "application/json",
		};
	}
}

// å¯¼å‡ºå•ä¾‹
export const chatService = new ChatService();

// å¯¼å‡ºç±»ä»¥ä¾¿æµ‹è¯•
export { ChatService };
